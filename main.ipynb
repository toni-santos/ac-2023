{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basketball Playoffs Qualification\n",
    "\n",
    "## Task description\n",
    "\n",
    "Basketball tournaments are usually split in two parts. First, all teams play each other aiming to achieve the greatest number of wins possible. Then, at the end of the first part of the season, a pre determined number of teams which were able to win the most games are qualified to the playoff season, where they play series of knock-out matches for the trophy.\n",
    "\n",
    "For the 10 years, data from players, teams, coaches, games and several other metrics were gathered and arranged on this dataset. The goal is to use this data to predict which teams will qualify for the playoffs in the next season.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "### Creating the database\n",
    "\n",
    "First, we need to convert the CSV files to tables in an SQLite database, so we can analyze, manipulate and prepare data more easily. This was done with a couple of SQlite3 commands:\n",
    "\n",
    "```\n",
    ".mode csv\n",
    ".import dataset/awards_players.csv awards_players\n",
    ".import dataset/coaches.csv coaches\n",
    ".import dataset/players.csv players\n",
    ".import dataset/players_teams.csv players_teams\n",
    ".import dataset/series_post.csv series_post\n",
    ".import dataset/teams_post.csv teams_post\n",
    ".import dataset/teams.csv teams\n",
    ".save database.db\n",
    "```\n",
    "\n",
    "### Filtering unneeded rows and columns\n",
    "\n",
    "Upon closer inspection of the dataset, we found some rows which had no effect or could have a negative impact in our models training, such as rows in the players table which corresponded to current coaches, and thus had no information related to their height, weight, etc.\n",
    "\n",
    "## Model performance measures\n",
    "\n",
    "### The Game Score measure\n",
    "The Game Score measure, created by John Hollinger, attempts to give an estimation of a player's productivity for a single game. We will start working on our model based on this measure, applying it to each player based on a whole season's stats and dividing it by the amount of games played.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframes based on the database and relations between data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"database.db\")\n",
    "\n",
    "# Player <-> Awards\n",
    "pl_aw = pd.read_sql_query('''\n",
    "    SELECT players_teams.playerID, players_teams.tmID,\n",
    "        awards_players.award, awards_players.year\n",
    "    FROM awards_players \n",
    "    LEFT JOIN players_teams\n",
    "    ON (\n",
    "        awards_players.playerID = players_teams.playerID \n",
    "        AND awards_players.year = players_teams.year\n",
    "    )''', con)\n",
    "\n",
    "# Coach <-> Awards\n",
    "cc_aw = pd.read_sql_query('''\n",
    "        SELECT playerID, award, c.year, c.tmID\n",
    "        FROM awards_players\n",
    "        INNER JOIN\n",
    "        (\n",
    "                SELECT coaches.coachID, teams.year,\n",
    "                    coaches.year, teams.tmID\n",
    "                FROM teams\n",
    "                INNER JOIN coaches\n",
    "                ON (\n",
    "                    coaches.tmID = teams.tmID\n",
    "                    AND coaches.year = teams.year\n",
    "                )    \n",
    "        ) AS c\n",
    "        ON (\n",
    "            awards_players.playerID = c.coachID\n",
    "            AND awards_players.year = c.year\n",
    "        )\n",
    "    ''', con)\n",
    "\n",
    "# Players\n",
    "pl = pd.read_sql_query(\"SELECT * FROM players\", con)\n",
    "\n",
    "# Teams\n",
    "tm = pd.read_sql_query(\"SELECT * FROM teams\", con)\n",
    "\n",
    "# Player Teams\n",
    "pt = pd.read_sql_query(\"SELECT * FROM players_teams\", con)\n",
    "\n",
    "# Player <-> Teams\n",
    "pl_tm = pd.read_sql_query(\"SELECT * FROM players_teams INNER JOIN players ON players_teams.playerID = players.bioID\", con)\n",
    "\n",
    "# Teams <-> Post Season Results (aggregated)\n",
    "tm_psa = pd.read_sql_query('''\n",
    "    SELECT teams.year, teams.lgID, teams.tmID, franchID,\n",
    "       confID, divID, rank, playoff, seeded, firstRound, semis,\n",
    "       finals, name, o_fgm, o_fga, o_ftm, o_fta, o_3pm, o_3pa,\n",
    "       o_oreb, o_dreb, o_reb, o_asts, o_pf, o_stl, o_to, o_blk,\n",
    "       o_pts, d_fgm, d_fga, d_ftm, d_fta, d_3pm, d_3pa, d_oreb,\n",
    "       d_dreb, d_reb, d_asts, d_pf, d_stl, d_to, d_blk, d_pts,\n",
    "       tmORB, tmDRB, tmTRB, opptmORB, opptmDRB, opptmTRB, won,\n",
    "       lost, GP, homeW, homeL, awayW, awayL, confW, confL,\n",
    "       min, attend, arena,W, L\n",
    "    FROM teams_post \n",
    "    INNER JOIN teams \n",
    "    ON (\n",
    "        teams_post.tmID = teams.tmID \n",
    "        AND teams_post.year = teams.year\n",
    "    )''', con)\n",
    "\n",
    "# Coach <-> Teams\n",
    "cc_tm = pd.read_sql_query(\"SELECT * FROM coaches INNER JOIN teams ON (coaches.tmID = teams.tmID AND coaches.year = teams.year)\", con)\n",
    "\n",
    "# Teams <-> Post Series Results\n",
    "tm_pss = pd.read_sql_query('''\n",
    "    SELECT winners.winnersID, winners.year, winners.winnersPlayoff, winners.winnersRank, losers.tmID, losers.playoff, losers.rank\n",
    "    FROM\n",
    "    (\n",
    "        SELECT teams.tmID AS winnersID, teams.year AS year, teams.playoff AS winnersPlayoff, teams.rank AS winnersRank, series_post.tmIDLoser AS tmIDLoser\n",
    "        FROM series_post \n",
    "        INNER JOIN teams\n",
    "        ON\n",
    "        (series_post.tmIDWinner = teams.tmID AND series_post.year = teams.year)\n",
    "    ) AS winners\n",
    "    JOIN teams AS losers\n",
    "    ON\n",
    "    (winners.tmIDLoser = losers.tmID AND winners.year = losers.year)\n",
    "''', con)\n",
    "cc_aw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column dropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remove columns that only have null values or no unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pl, pl_tm, tm_psa, cc_tm, tm]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    unique_counts = dataframes[i].nunique()\n",
    "    dropped_columns = dataframes[i].columns[(dataframes[i].isna().sum() == len(dataframes[i])) | (unique_counts == 1)]\n",
    "    \n",
    "    print(f\"Dropped columns in dataframe {i}: {list(dropped_columns)}\")\n",
    "    \n",
    "    dataframes[i] = dataframes[i].drop(columns=dropped_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove Player rows that have birth-date `0000-00-00`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pl.drop(pl[pl['birthDate'] == '0000-00-00'].index, axis = 0)\n",
    "pl_tm = pl_tm.drop(pl_tm[pl_tm['birthDate'] == '0000-00-00'].index, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, when applicable, we remove the Team `name` attribute, since we already have access to the `tmID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_psa = tm_psa.drop(['name'], axis=1)\n",
    "cc_tm = cc_tm.drop(['name'], axis=1)\n",
    "tm = tm.drop(['name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical encoding of Awards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since not all awards are equal, it's useful to attribute each one a score, to signal to the algorithms the relevance each one has. For example, the `All-Star Game Most Valuable Player` is the most valuable one, while the `Kim Perrot Sportsmanship Award` is attributed only to those who show sportsmanship, revealing no real playing skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl_aw['award'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the `Kim Perrot Sportsmanship Award` has two possible values, so we merge those two, before encoding the awards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_aw['award'] = pl_aw['award'].replace(['Kim Perrot Sportsmanship', 'Kim Perrot Sportsmanship Award'], 'Kim Perrot Sportsmanship Award')\n",
    "\n",
    "print(pl_aw['award'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rank the awards as follows:\n",
    "\n",
    "| Award                                    | Score |\n",
    "|------------------------------------------|-------|\n",
    "| 'All-Star Game Most Valuable Player'     |    7  |\n",
    "| 'Coach of the Year'                      |   10  |\n",
    "| 'Defensive Player of the Year'           |    7  |\n",
    "| 'Kim Perrot Sportsmanship Award'         |    3  |\n",
    "| 'Most Improved Player'                   |    8  |\n",
    "| 'Most Valuable Player'                   |   10  |\n",
    "| 'Rookie of the Year'                     |    8  |\n",
    "| 'Sixth Woman of the Year'                |    7  |\n",
    "| 'WNBA Finals Most Valuable Player'       |    7  |\n",
    "| 'WNBA All-Decade Team'                   |    4  |\n",
    "| 'WNBA All Decade Team Honorable Mention' |    3  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_scores = {\n",
    "    'All-Star Game Most Valuable Player': 7,\n",
    "    'Coach of the Year': 10,\n",
    "    'Defensive Player of the Year': 7,\n",
    "    'Kim Perrot Sportsmanship Award': 3,\n",
    "    'Most Improved Player': 8,\n",
    "    'Most Valuable Player': 10,\n",
    "    'Rookie of the Year': 8,\n",
    "    'Sixth Woman of the Year': 7,\n",
    "    'WNBA Finals Most Valuable Player': 7,\n",
    "    'WNBA All-Decade Team': 4,\n",
    "    'WNBA All Decade Team Honorable Mention': 3\n",
    "}\n",
    "\n",
    "pl_aw['award_score'] = pl_aw['award'].map(award_scores)\n",
    "cc_aw['award_score'] = cc_aw['award'].map(award_scores)\n",
    "\n",
    "print(pl_aw)\n",
    "print(cc_aw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the graphs of this metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_award_scores = pl_aw.groupby(['tmID', 'year'])['award_score'].sum().reset_index()\n",
    "\n",
    "# Merge team award scores with team wins\n",
    "merged_team_data = pd.merge(team_award_scores, tm_psa[['tmID', 'year', 'won']], on=['tmID', 'year'], how='left')\n",
    "merged_team_data['year'] = pd.to_numeric(merged_team_data['year'])\n",
    "merged_team_data.sort_values(by='year', inplace=True)\n",
    "\n",
    "merged_team_data['award_score'] = pd.to_numeric(merged_team_data['award_score'])\n",
    "merged_team_data['won'] = pd.to_numeric(merged_team_data['won'])\n",
    "\n",
    "# Create a scatter plot with regression line for each year\n",
    "g = sns.FacetGrid(merged_team_data, col=\"year\", col_wrap=4, height=4, sharex=False)\n",
    "g.map(sns.scatterplot, 'award_score', 'won', alpha=0.7)\n",
    "#g.map(sns.regplot, 'award_score', 'won', scatter=False, color='red')  # Add regression line\n",
    "g.set_axis_labels(\"Team's Award Score\", \"Number of Wins\")\n",
    "g.set_titles(\"Year {col_name}\")\n",
    "g.add_legend(title='Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to analyse the `weight` and `height` attributes' Z-Score and IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers for Player's Weight + Height\n",
    "\n",
    "# Convert Height and Weight Columns into numeric values\n",
    "pl['height'] = pd.to_numeric(pl['height'], errors='coerce')\n",
    "pl['weight'] = pd.to_numeric(pl['weight'], errors='coerce')\n",
    "\n",
    "pl_tm['height'] = pd.to_numeric(pl_tm['height'], errors='coerce')\n",
    "pl_tm['weight'] = pd.to_numeric(pl_tm['weight'], errors='coerce')\n",
    "\n",
    "pl = pl.dropna(subset=['height', 'weight'])\n",
    "pl_tm = pl_tm.dropna(subset=['height', 'weight'])\n",
    "\n",
    "height_zscores = stats.zscore(pl['height'])\n",
    "weight_zscores = stats.zscore(pl['weight'])\n",
    "\n",
    "# Plot boxplot\n",
    "print(\"Height Z-Score\\n\", height_zscores)\n",
    "print(\"\\nWeight Z-Score\\n\", weight_zscores)\n",
    "\n",
    "plt.title('Height Distribution')\n",
    "plt.boxplot(pl['height'])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Weight Distribution')\n",
    "plt.boxplot(pl['weight'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier for Team's played minutes\n",
    "\n",
    "tm_psa['min'] = pd.to_numeric(tm_psa['min'], errors='coerce')\n",
    "cc_tm['min'] = pd.to_numeric(cc_tm['min'], errors='coerce')\n",
    "\n",
    "tm_psa = tm_psa.dropna(subset=['min'])\n",
    "cc_tm = cc_tm.dropna(subset=['min'])\n",
    "\n",
    "\n",
    "print(\"Team Played minutes Z-Score\\n\", stats.zscore(tm_psa['min']))\n",
    "plt.title('Team played minutes Distribution')\n",
    "plt.boxplot(tm_psa['min'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier for Player's played minutes\n",
    "\n",
    "pl_tm['minutes'] = pd.to_numeric(pl_tm['minutes'], errors='coerce')\n",
    "\n",
    "print(\"Player Played minutes Z-Score\\n\", stats.zscore(pl_tm['minutes']))\n",
    "plt.title('Player played minutes Distribution')\n",
    "plt.boxplot(pl_tm['minutes'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Team points\n",
    "\n",
    "tm_psa['o_pts'] = pd.to_numeric(tm_psa['o_pts'], errors='coerce')\n",
    "\n",
    "print(\"Team's Offense Score Z-Score\\n\", stats.zscore(tm_psa['o_pts']))\n",
    "plt.title(\"Team's Offence Score Distribution\")\n",
    "plt.boxplot(tm_psa['o_pts'])\n",
    "plt.show()\n",
    "\n",
    "tm_psa['d_pts'] = pd.to_numeric(tm_psa['d_pts'], errors='coerce')\n",
    "\n",
    "print(\"Team's Defense Score Z-Score\\n\", stats.zscore(tm_psa['d_pts']))\n",
    "plt.title(\"Team's Defense Score Distribution\")\n",
    "plt.boxplot(tm_psa['d_pts'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Player points\n",
    "\n",
    "pl_tm['points'] = pd.to_numeric(pl_tm['points'], errors='coerce')\n",
    "\n",
    "print(\"Player's Scores Z-Score\\n\", stats.zscore(pl_tm['points']))\n",
    "plt.title(\"Player's Scores Distribution\")\n",
    "plt.boxplot(pl_tm['points'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we didn't find it useful to remove the outliers, since they represent players that exceed average measures, as opposed to mistakes in the data collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_mat(df, name, columns):\n",
    "    correlation_matrix = df[columns].corr()\n",
    "    \n",
    "    plt.figure(figsize=(17, 10))\n",
    "\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "    sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, annot=True, fmt=\".2f\")\n",
    "    plt.title(f'Correlation Matrix {name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pl_tm, tm_psa]\n",
    "names = ['pl_tm', 'tm_psa']\n",
    "\n",
    "selected_columns = [\n",
    "\t[\n",
    "       'oRebounds', 'dRebounds', 'rebounds', 'assists', 'steals', 'blocks',\n",
    "       'turnovers', 'PF', 'fgAttempted', 'fgMade', 'ftAttempted', 'ftMade',\n",
    "       'threeAttempted', 'threeMade', 'dq', 'PostGP', 'PostGS', 'PostMinutes',\n",
    "       'PostPoints', 'PostoRebounds', 'PostdRebounds', 'PostRebounds',\n",
    "       'PostAssists', 'PostSteals', 'PostBlocks', 'PostTurnovers', 'PostPF',\n",
    "       'PostfgAttempted', 'PostfgMade', 'PostftAttempted', 'PostftMade',\n",
    "       'PostthreeAttempted', 'PostthreeMade', 'PostDQ'],\n",
    "\t\n",
    "\t[ 'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm', 'o_3pa',\n",
    "       'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_pf', 'o_stl', 'o_to', 'o_blk',\n",
    "       'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm', 'd_3pa', 'd_oreb',\n",
    "       'd_dreb', 'd_reb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk', 'd_pts',\n",
    "       'min']\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(0, len(dataframes)):\n",
    "\tdf = dataframes[i]\n",
    "\tfor column in selected_columns[i]:\n",
    "\t\tif (df[column].dtype == 'object'):  # Check if the column contains strings\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf[column] = pd.to_numeric(df[column])  # Try to convert to numeric\n",
    "\t\t\texcept ValueError:\n",
    "\t\t\t\tpass\n",
    "    \n",
    "\t\n",
    "\tplot_corr_mat(df, names[i], selected_columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframe, `df`, to be used with the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tm\n",
    "df['year'] = df['year'].astype(int)\n",
    "df[\"playoff\"].replace({\"N\": 0, \"Y\": 1}, inplace=True)\n",
    "df.sort_values(by=['year'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge columns with performance data into a single performance indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game Score, applied to the season and to the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for teams with a strong defensive performance to have their strengths represented a \"defensive game score\", `def_metric_game_score`, is also being calculated and the average of the scores used as the final metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['o_pts', 'o_fgm', 'o_fga', 'o_3pm', 'o_fta', 'o_ftm', 'o_oreb', 'o_dreb', 'o_stl', 'o_asts', 'o_blk', 'o_pf', 'o_to', 'GP']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "for col in ['d_pts', 'd_fgm', 'd_fga', 'd_3pm', 'd_fta', 'd_ftm', 'd_oreb', 'd_dreb', 'd_stl', 'd_asts', 'd_blk', 'd_pf', 'd_to', 'GP']:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "df['off_metric_game_score'] = (df['o_pts'] + 0.4 * df['o_fgm'] - 0.7 * df['o_fga'] - 0.4 * (df['o_fta'] - df['o_ftm']) + 0.7 * df['o_oreb'] + 0.3 * df['o_dreb'] + df['o_stl'] + 0.7 * df['o_asts'] + 0.7 * df['o_blk'] - 0.4 * df['o_pf'] - df['o_to']) / df['GP']\n",
    "df['def_metric_game_score'] = (df['d_pts'] + 0.4 * df['d_fgm'] - 0.7 * df['d_fga'] - 0.4 * (df['d_fta'] - df['d_ftm']) + 0.7 * df['d_oreb'] + 0.3 * df['d_dreb'] + df['d_stl'] + 0.7 * df['d_asts'] + 0.7 * df['d_blk'] - 0.4 * df['d_pf'] - df['d_to']) / df['GP']\n",
    "df['metric_game_score'] = (df['off_metric_game_score'] + df['def_metric_game_score'])/2\n",
    "\n",
    "print(df.sort_values(by='metric_game_score', ascending=False)['metric_game_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective Field Goal (%) and Free Throw Rate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eFG%'] = (df['o_fgm'] + 0.5 * df['o_3pm']) / df['o_fga']\n",
    "df['FTA_rate'] = df['o_fta'] / df['o_fga']\n",
    "df.sort_values(by='year', ascending=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot to show relation between number of wins and effective field goal percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = df.copy()\n",
    "\n",
    "copy['eFG%'] = pd.to_numeric(copy['eFG%'], errors='coerce')  # Convert 'eFG%' to numeric type if needed\n",
    "copy['won'] = pd.to_numeric(copy['won'], errors='coerce')  # Convert 'won' to numeric type if needed\n",
    "\n",
    "copy.sort_values(by='won', inplace=True)\n",
    "\n",
    "# Create a scatter plot for each year\n",
    "g = sns.FacetGrid(copy, col=\"year\", col_wrap=4, height=4, sharex=False)\n",
    "g.map(sns.scatterplot, 'eFG%', 'won', alpha=0.7)\n",
    "g.map(sns.regplot, 'eFG%', 'won', scatter=False, color='red')  # Add regression line\n",
    "g.set_axis_labels(\"Effective Field Goal Percentage (eFG%)\", \"Number of Wins\")\n",
    "g.set_titles(\"Year {col_name}\")\n",
    "g.add_legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game Score, applied to the season and to the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['points', 'fgMade', 'fgAttempted', 'ftAttempted', 'ftMade', 'oRebounds', 'dRebounds', 'steals', 'assists', 'blocks', 'PF', 'turnovers', 'GP']:\n",
    "    pl_tm[col] = pl_tm[col].astype(int)\n",
    "\n",
    "pl_tm['off_metric_game_score'] = (pl_tm['points'] + 0.4 * pl_tm['fgMade'] - 0.7 * pl_tm['fgAttempted'] - 0.4 * (pl_tm['ftAttempted'] - pl_tm['ftMade']) + 0.7 * pl_tm['oRebounds'] + 0.3 * pl_tm['dRebounds'] + pl_tm['steals'] + 0.7 * pl_tm['assists'] + 0.7 * pl_tm['blocks'] - 0.4 * pl_tm['PF'] - pl_tm['turnovers']) / pl_tm['GP']\n",
    "print(pl_tm.sort_values(by='off_metric_game_score', ascending=False)['off_metric_game_score'])\n",
    "\n",
    "mean_gs = pl_tm.groupby(['tmID', 'year'])[\"off_metric_game_score\"].mean().reset_index().sort_values(by='off_metric_game_score', ascending=False)\n",
    "\n",
    "for idx, x in mean_gs.iterrows():\n",
    "    year_condition = df['year'] == int(x['year'])\n",
    "    tmID_condition = df['tmID'] == str(x['tmID'])\n",
    "    df.loc[year_condition & tmID_condition, \"mean_player_game_score\"] = x['off_metric_game_score']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot between Game Score and Wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_tm['off_metric_game_score'] = pd.to_numeric(pl_tm['off_metric_game_score'], errors='coerce')  # Convert to numeric if needed\n",
    "\n",
    "# Sum the 'off_metric_game_score' for each team's players\n",
    "team_off_metric_scores = pl_tm.groupby(['tmID', 'year'])['off_metric_game_score'].sum().reset_index()\n",
    "\n",
    "# Convert 'tmID' and 'year' columns to the same data type as in 'df'\n",
    "team_off_metric_scores['tmID'] = team_off_metric_scores['tmID'].astype(copy['tmID'].dtype)\n",
    "team_off_metric_scores['year'] = team_off_metric_scores['year'].astype(copy['year'].dtype)\n",
    "\n",
    "# Merge with 'df' to get 'won' column\n",
    "merged_data = pd.merge(team_off_metric_scores, copy[['tmID', 'year', 'won']], on=['tmID', 'year'], how='left')\n",
    "\n",
    "# Sort the DataFrame by 'won'\n",
    "merged_data.sort_values(by='won', inplace=True)\n",
    "\n",
    "# Create a scatter plot for each year\n",
    "g = sns.FacetGrid(merged_data, col=\"year\", col_wrap=4, height=4, sharex=False)\n",
    "g.map(sns.scatterplot, 'off_metric_game_score', 'won', alpha=0.7)\n",
    "g.map(sns.regplot, 'off_metric_game_score', 'won', scatter=False, color='red')  # Add regression line\n",
    "g.set_axis_labels(\"Sum of Player Off Metric Game Score\", \"Number of Wins\")\n",
    "g.set_titles(\"Year {col_name}\")\n",
    "g.add_legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an award score for each `team-year` occurence based on the award enconding previously employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_player_award_score = pl_aw.groupby(['tmID', 'year'])['award_score'].sum().reset_index()\n",
    "team_coach_award_score = cc_aw.groupby(['tmID', 'year'])['award_score'].sum().reset_index()\n",
    "\n",
    "df['award_score'] = 0\n",
    "\n",
    "for idx, x in team_player_award_score.iterrows():\n",
    "    year_condition = df['year'] == int(x['year'])\n",
    "    tmID_condition = df['tmID'] == str(x['tmID'])\n",
    "    df.loc[year_condition & tmID_condition, \"award_score\"] += x['award_score']\n",
    "\n",
    "for idx, x in team_coach_award_score.iterrows():\n",
    "    year_condition = df['year'] == int(x['year'])\n",
    "    tmID_condition = df['tmID'] == str(x['tmID'])\n",
    "    df.loc[year_condition & tmID_condition, \"award_score\"] += x['award_score']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt['year'] = pt['year'].astype(int)\n",
    "pt.sort_values(by=['year'], inplace=True)\n",
    "grouped = pt.groupby(['tmID', 'year'])[['tmID', 'playerID', 'year']]\n",
    "teams_dict = {}\n",
    "res_dict = {}\n",
    "\n",
    "for name, group in grouped:\n",
    "    loop_df = pd.DataFrame(group)\n",
    "    if name[0] in teams_dict.keys():\n",
    "        teams_dict[name[0]][name[1]] = loop_df['playerID'].unique().tolist()\n",
    "    else:\n",
    "        teams_dict[name[0]] = {}\n",
    "        teams_dict[name[0]][name[1]] = loop_df['playerID'].unique().tolist()\n",
    "\n",
    "for x in teams_dict.keys():\n",
    "    prev = None\n",
    "    if len(teams_dict[x].keys()) == 1:\n",
    "        res_dict[x][teams_dict[x].keys()[0]] = 0\n",
    "        continue\n",
    "    for y in teams_dict[x].keys():\n",
    "        if prev == None: \n",
    "            prev = y\n",
    "            res_dict[x] = {}\n",
    "            res_dict[x][y] = 0\n",
    "            continue\n",
    "\n",
    "        past = teams_dict[x][prev]\n",
    "        present = teams_dict[x][y]\n",
    "        prev = y\n",
    "        count = 0\n",
    "\n",
    "        for player in past:\n",
    "            if player in present:\n",
    "                count += 1\n",
    "\n",
    "        # ratio of players that stayed\n",
    "        count /= len(present)\n",
    "\n",
    "        res_dict[x][y] = {}\n",
    "        res_dict[x][y] = round(count,2)\n",
    "\n",
    "\n",
    "for key in res_dict.keys():\n",
    "    for year in res_dict[key].keys():\n",
    "        year_condition = df['year'] == int(year)\n",
    "        tmID_condition = df['tmID'] == str(key)\n",
    "        df.loc[year_condition & tmID_condition, \"player_retention\"] = res_dict[key][year]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player retention calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt['year'] = pt['year'].astype(int)\n",
    "pt.sort_values(by=['year'], inplace=True)\n",
    "grouped = pt.groupby(['tmID', 'year'])[['tmID', 'playerID', 'year']]\n",
    "teams_dict = {}\n",
    "res_dict = {}\n",
    "\n",
    "\n",
    "for name, group in grouped:\n",
    "    loop_df = pd.DataFrame(group)\n",
    "    if name[0] in teams_dict.keys():\n",
    "        teams_dict[name[0]][name[1]] = loop_df['playerID'].unique().tolist()\n",
    "    else:\n",
    "        teams_dict[name[0]] = {}\n",
    "        teams_dict[name[0]][name[1]] = loop_df['playerID'].unique().tolist()\n",
    "\n",
    "for x in teams_dict.keys():\n",
    "    prev = None\n",
    "    if len(teams_dict[x].keys()) == 1:\n",
    "        res_dict[x][teams_dict[x].keys()[0]] = 0\n",
    "        continue\n",
    "    for y in teams_dict[x].keys():\n",
    "        if prev == None: \n",
    "            prev = y\n",
    "            res_dict[x] = {}\n",
    "            res_dict[x][y] = 0\n",
    "            continue\n",
    "\n",
    "        past = teams_dict[x][prev]\n",
    "        present = teams_dict[x][y]\n",
    "        prev = y\n",
    "        count = 0\n",
    "\n",
    "        for player in past:\n",
    "            if player in present:\n",
    "                count += 1\n",
    "\n",
    "        # ratio of players that stayed\n",
    "        count /= len(present)\n",
    "\n",
    "        res_dict[x][y] = {}\n",
    "        res_dict[x][y] = round(count,2)\n",
    "\n",
    "\n",
    "for key in res_dict.keys():\n",
    "    for year in res_dict[key].keys():\n",
    "        year_condition = df['year'] == int(year)\n",
    "        tmID_condition = df['tmID'] == str(key)\n",
    "        df.loc[year_condition & tmID_condition, \"player_retention\"] = res_dict[key][year]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features have different orders of magnitude. Since some models benefit from standardized scaling of features, we will use a `MinMaxScaler` to transform our created features between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "feature_cols = [\"def_metric_game_score\", \"metric_game_score\", \"off_metric_game_score\", \"eFG%\", \"FTA_rate\", \"award_score\", \"player_retention\", \"mean_player_game_score\"]\n",
    "\n",
    "scaled_df = df[feature_cols]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(scaled_df), index=scaled_df.index, columns=scaled_df.columns)\n",
    "for col in feature_cols:\n",
    "    df[col] = scaled_df[col]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to accurately predict the following year playoffs qualification, we must analyse the previous year performance. So, we will shift the data one year. This way, data marked from the end of year 1, will be used as learning data for the start of year 2.\n",
    "\n",
    "Instead of shifting every relevant column one year ahead, we will instead increment the year value by one: this way, the year 1 results will be in the year 2 row, which mimics the effect we want. We will save year 10's results (which would be placed under year 11) and delete them from the dataframe, as they will only be used when making the final prediction (for year 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = (df['year'].astype(int) + 1)\n",
    "year_10_df = df[df['year'] == 11]\n",
    "year_10_df.to_csv('dataset/teams_year_10.csv', index=None)\n",
    "df = df[df['year'] < 11]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, roc_auc_score\n",
    "feature_selection_func = LinearSVC(dual=\"auto\", penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test set balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dictionary to hold the train/test split values in order to emulate different model training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = {\n",
    "    5/9: 4/9,\n",
    "    6/9: 3/9,\n",
    "    8/9: 1/9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', DecisionTreeClassifier(random_state=48))])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[[\"playoff\"]]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "dt_acc = 0\n",
    "dt_prec = 0\n",
    "dt_f1 = 0\n",
    "dt_rec = 0\n",
    "dt_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    dt_acc += accuracy_score(y_test, y_prediction)\n",
    "    dt_prec += precision_score(y_test, y_prediction)\n",
    "    dt_f1 += f1_score(y_test, y_prediction)\n",
    "    dt_rec += recall_score(y_test, y_prediction)\n",
    "    dt_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    [print(f\"{trained_model.feature_names_in_[idx]}: {round(x*100, 2)}%\") for idx, x in enumerate(trained_model['classification'].feature_importances_)]\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "dt_acc /= len(train_test.keys())\n",
    "dt_prec /= len(train_test.keys())\n",
    "dt_f1 /= len(train_test.keys())\n",
    "dt_rec /= len(train_test.keys())\n",
    "dt_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(dt_acc)\n",
    "print(dt_prec)\n",
    "print(dt_f1)\n",
    "print(dt_rec)\n",
    "print(dt_roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Gaussian and Mulitnomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', GaussianNB())])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "nbg_acc = 0\n",
    "nbg_prec = 0\n",
    "nbg_f1 = 0\n",
    "nbg_rec = 0\n",
    "nbg_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    nbg_acc += accuracy_score(y_test, y_prediction)\n",
    "    nbg_prec += precision_score(y_test, y_prediction)\n",
    "    nbg_f1 += f1_score(y_test, y_prediction)\n",
    "    nbg_rec += recall_score(y_test, y_prediction)\n",
    "    nbg_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "nbg_acc /= len(train_test.keys())\n",
    "nbg_prec /= len(train_test.keys())\n",
    "nbg_f1 /= len(train_test.keys())\n",
    "nbg_rec /= len(train_test.keys())\n",
    "nbg_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(nbg_acc)\n",
    "print(nbg_prec)\n",
    "print(nbg_f1)\n",
    "print(nbg_rec)\n",
    "print(nbg_roc_auc)\n",
    "\n",
    "print()\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', MultinomialNB())])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "nbm_acc = 0\n",
    "nbm_prec = 0\n",
    "nbm_f1 = 0\n",
    "nbm_rec = 0\n",
    "nbm_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    nbm_acc += accuracy_score(y_test, y_prediction)\n",
    "    nbm_prec += precision_score(y_test, y_prediction)\n",
    "    nbm_f1 += f1_score(y_test, y_prediction)\n",
    "    nbm_rec += recall_score(y_test, y_prediction)\n",
    "    nbm_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "nbm_acc /= len(train_test.keys())\n",
    "nbm_prec /= len(train_test.keys())\n",
    "nbm_f1 /= len(train_test.keys())\n",
    "nbm_rec /= len(train_test.keys())\n",
    "nbm_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(nbm_acc)\n",
    "print(nbm_prec)\n",
    "print(nbm_f1)\n",
    "print(nbm_rec)\n",
    "print(nbm_roc_auc)\n",
    "# [print(f\"{trained_model.feature_names_in_[idx]}: {x}\") for idx, x in enumerate(trained_model.feature_importances_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', KNeighborsClassifier())])\n",
    "\n",
    "\n",
    "parameter_combinations = {\n",
    "    'classification__n_neighbors': [*range(3, 16, 2)],\n",
    "    'classification__weights': ['uniform', 'distance'],\n",
    "    'classification__metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'classification__algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'classification__p': [1, 2]\n",
    "}\n",
    "\n",
    "pipeline = GridSearchCV(pipeline, parameter_combinations, cv=5, scoring='roc_auc')\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "knn_acc = 0\n",
    "knn_prec = 0\n",
    "knn_f1 = 0\n",
    "knn_rec = 0\n",
    "knn_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "    print(pipeline.best_params_)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    knn_acc += accuracy_score(y_test, y_prediction)\n",
    "    knn_prec += precision_score(y_test, y_prediction)\n",
    "    knn_f1 += f1_score(y_test, y_prediction)\n",
    "    knn_rec += recall_score(y_test, y_prediction)\n",
    "    knn_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "knn_acc /= len(train_test.keys())\n",
    "knn_prec /= len(train_test.keys())\n",
    "knn_f1 /= len(train_test.keys())\n",
    "knn_rec /= len(train_test.keys())\n",
    "knn_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(knn_acc)\n",
    "print(knn_prec)\n",
    "print(knn_f1)\n",
    "print(knn_rec)\n",
    "print(knn_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', RandomForestClassifier(random_state=48))])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "parameter_combinations = {\n",
    "    'classification__n_estimators': [50, 100, 200],\n",
    "    'classification__min_samples_split': [2, 4, 9],\n",
    "    'classification__min_samples_leaf': [2, 3, 5],\n",
    "    'classification__max_features': ['sqrt', 'log2'],\n",
    "    'classification__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "pipeline = GridSearchCV(pipeline, parameter_combinations, cv=5, scoring='roc_auc')\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "rf_acc = 0\n",
    "rf_prec = 0\n",
    "rf_f1 = 0\n",
    "rf_rec = 0\n",
    "rf_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "    print(pipeline.best_params_)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    rf_acc += accuracy_score(y_test, y_prediction)\n",
    "    rf_prec += precision_score(y_test, y_prediction)\n",
    "    rf_f1 += f1_score(y_test, y_prediction)\n",
    "    rf_rec += recall_score(y_test, y_prediction)\n",
    "    rf_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "rf_acc /= len(train_test.keys())\n",
    "rf_prec /= len(train_test.keys())\n",
    "rf_f1 /= len(train_test.keys())\n",
    "rf_rec /= len(train_test.keys())\n",
    "rf_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(rf_acc)\n",
    "print(rf_prec)\n",
    "print(rf_f1)\n",
    "print(rf_rec)\n",
    "print(rf_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') # ignore warnings about l1_ratio not being used outside elasticnet penalty\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', LogisticRegression(random_state=48))])\n",
    "\n",
    "parameter_combinations = {\n",
    "    'classification__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classification__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'classification__l1_ratio': [0.2, 0.4, 0.6, 0.8],\n",
    "    'classification__solver': ['saga'],\n",
    "    'classification__max_iter': [1000, 3000, 5000]\n",
    "}\n",
    "\n",
    "pipeline = GridSearchCV(pipeline, parameter_combinations, cv=5, scoring='roc_auc')\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "\n",
    "lr_acc = 0\n",
    "lr_prec = 0\n",
    "lr_f1 = 0\n",
    "lr_rec = 0\n",
    "lr_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "    print(pipeline.best_params_)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    lr_acc += accuracy_score(y_test, y_prediction)\n",
    "    lr_prec += precision_score(y_test, y_prediction)\n",
    "    lr_f1 += f1_score(y_test, y_prediction)\n",
    "    lr_rec += recall_score(y_test, y_prediction)\n",
    "    lr_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "lr_acc /= len(train_test.keys())\n",
    "lr_prec /= len(train_test.keys())\n",
    "lr_f1 /= len(train_test.keys())\n",
    "lr_rec /= len(train_test.keys())\n",
    "lr_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(lr_acc)\n",
    "print(lr_prec)\n",
    "print(lr_f1)\n",
    "print(lr_rec)\n",
    "print(lr_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', SVC(random_state=48))])\n",
    "\n",
    "parameter_combinations = {\n",
    "    'classification__C': [0.1, 1, 10],\n",
    "    'classification__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'classification__degree': [2, 3, 4],\n",
    "    'classification__gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'classification__shrinking': [True, False],\n",
    "    'classification__probability': [True, False]\n",
    "}\n",
    "\n",
    "pipeline = GridSearchCV(pipeline, parameter_combinations, cv=5, scoring='roc_auc')\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "svc_acc = 0\n",
    "svc_prec = 0\n",
    "svc_f1 = 0\n",
    "svc_rec = 0\n",
    "svc_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "    print(pipeline.best_params_)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    svc_acc += accuracy_score(y_test, y_prediction)\n",
    "    svc_prec += precision_score(y_test, y_prediction)\n",
    "    svc_f1 += f1_score(y_test, y_prediction)\n",
    "    svc_rec += recall_score(y_test, y_prediction)\n",
    "    svc_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "svc_acc /= len(train_test.keys())\n",
    "svc_prec /= len(train_test.keys())\n",
    "svc_f1 /= len(train_test.keys())\n",
    "svc_rec /= len(train_test.keys())\n",
    "svc_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(svc_acc)\n",
    "print(svc_prec)\n",
    "print(svc_f1)\n",
    "print(svc_rec)\n",
    "print(svc_roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', GradientBoostingClassifier(random_state=48))])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "gbt_acc = 0\n",
    "gbt_prec = 0\n",
    "gbt_f1 = 0\n",
    "gbt_rec = 0\n",
    "gbt_roc_auc = 0\n",
    "\n",
    "for train, test in train_test.items():\n",
    "    # Fit the model to the training data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_file, Y_file, train_size=train, test_size=test, shuffle=False, random_state=48)\n",
    "    trained_model = pipeline.fit(x_train, y_train)\n",
    "    print(f\"Train/test split: {train}/{test}\")\n",
    "\n",
    "    # Predict using the trained model\n",
    "    y_prediction = trained_model.predict(x_test)\n",
    "\n",
    "    gbt_acc += accuracy_score(y_test, y_prediction)\n",
    "    gbt_prec += precision_score(y_test, y_prediction)\n",
    "    gbt_f1 += f1_score(y_test, y_prediction)\n",
    "    gbt_rec += recall_score(y_test, y_prediction)\n",
    "    gbt_roc_auc += roc_auc_score(y_test, y_prediction)\n",
    "\n",
    "    print(accuracy_score(y_test, y_prediction))\n",
    "    print(precision_score(y_test, y_prediction))\n",
    "    print(f1_score(y_test, y_prediction))\n",
    "    print(recall_score(y_test, y_prediction))\n",
    "    print(roc_auc_score(y_test, y_prediction))\n",
    "\n",
    "    [print(f\"{trained_model.feature_names_in_[idx]}: {x}\") for idx, x in enumerate(trained_model['classification'].feature_importances_)]\n",
    "    \n",
    "    print(\"-\" * 18)\n",
    "\n",
    "gbt_acc /= len(train_test.keys())\n",
    "gbt_prec /= len(train_test.keys())\n",
    "gbt_f1 /= len(train_test.keys())\n",
    "gbt_rec /= len(train_test.keys())\n",
    "gbt_roc_auc /= len(train_test.keys())\n",
    "\n",
    "print(\"Overall:\")\n",
    "print(gbt_acc)\n",
    "print(gbt_prec)\n",
    "print(gbt_f1)\n",
    "print(gbt_rec)\n",
    "print(gbt_roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_dict = {'Accuracy': dt_acc,\n",
    "            'Precision': dt_prec,\n",
    "            'F1': dt_f1,\n",
    "            'Recall': dt_rec,\n",
    "            'Area under ROC curve': dt_roc_auc}\n",
    "\n",
    "nbg_dict = {'Accuracy': nbg_acc,\n",
    "            'Precision': nbg_prec,\n",
    "            'F1': nbg_f1,\n",
    "            'Recall': nbg_rec,\n",
    "            'Area under ROC curve': nbg_roc_auc}\n",
    "\n",
    "nbm_dict = {'Accuracy': nbm_acc,\n",
    "            'Precision': nbm_prec,\n",
    "            'F1': nbm_f1,\n",
    "            'Recall': nbm_rec,\n",
    "            'Area under ROC curve': nbm_roc_auc}\n",
    "\n",
    "knn_dict = {'Accuracy': knn_acc,\n",
    "            'Precision': knn_prec,\n",
    "            'F1': knn_f1,\n",
    "            'Recall': knn_rec,\n",
    "            'Area under ROC curve': knn_roc_auc}\n",
    "\n",
    "rf_dict = {'Accuracy': rf_acc,\n",
    "            'Precision': rf_prec,\n",
    "            'F1': rf_f1,\n",
    "            'Recall': rf_rec,\n",
    "            'Area under ROC curve': rf_roc_auc}\n",
    "\n",
    "lr_dict = {'Accuracy': lr_acc,\n",
    "            'Precision': lr_prec,\n",
    "            'F1': lr_f1,\n",
    "            'Recall': lr_rec,\n",
    "            'Area under ROC curve': lr_roc_auc}\n",
    "\n",
    "svc_dict = {'Accuracy': svc_acc,\n",
    "            'Precision': svc_prec,\n",
    "            'F1': svc_f1,\n",
    "            'Recall': svc_rec,\n",
    "            'Area under ROC curve': svc_roc_auc}\n",
    "\n",
    "gbt_dict = {'Accuracy': gbt_acc,\n",
    "            'Precision': gbt_prec,\n",
    "            'F1': gbt_f1,\n",
    "            'Recall': gbt_rec,\n",
    "            'Area under ROC curve': gbt_roc_auc}\n",
    "\n",
    "comparison = pd.DataFrame({'Decision Tree': pd.Series(dt_dict),\n",
    "                       'Gaussian Naive Bayes': pd.Series(nbg_dict),\n",
    "                       'Multinomial Naive Bayes': pd.Series(nbm_dict),\n",
    "                       'K-Nearest Neighbors': pd.Series(knn_dict),\n",
    "                       'Random Forest': pd.Series(rf_dict),\n",
    "                       'Logistic Regression': pd.Series(lr_dict),\n",
    "                       'Support Vector Machines': pd.Series(svc_dict),\n",
    "                       'Gradient Boosted Trees': pd.Series(gbt_dict),\n",
    "                      })\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv('dataset/teams_year_10.csv')\n",
    "year_11_df = pd.read_csv('dataset/teams_year_11.csv')\n",
    "\n",
    "year_11_teams = year_11_df['franchID'].values\n",
    "pred_df = pred_df[pred_df['franchID'].isin(year_11_teams)]\n",
    "\n",
    "pipeline = Pipeline([('feature_selection', SelectFromModel(feature_selection_func)),\n",
    "                    ('classification', LogisticRegression(C=100, max_iter=1000, penalty='l1', solver='saga', random_state=48))])\n",
    "\n",
    "X_file, Y_file = df.drop(\"playoff\", axis=1), df[\"playoff\"]\n",
    "\n",
    "for column in df.columns:\n",
    "    if (column not in feature_cols + [\"playoff\"]):\n",
    "        X_file.drop(column, axis=1, inplace=True)\n",
    "        pred_df.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "trained_model = pipeline.fit(X_file, Y_file)\n",
    "\n",
    "pred_df.drop('playoff', axis=1, inplace=True)\n",
    "predictions = trained_model.predict(pred_df)\n",
    "\n",
    "year_11_df['playoff'] = predictions\n",
    "year_11_df[\"playoff\"].replace({0: \"N\", 1: \"Y\"}, inplace=True)\n",
    "year_11_df = year_11_df[['name', 'playoff']]\n",
    "year_11_df.to_csv(\"dataset/prediction.csv\", index=None)\n",
    "year_11_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
